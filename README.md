# From-Vision-to-Sound-GenAI-Explorations-Between-Image-and-Soundscape
Project Overview

This project explores the potential of GenAI in translating visual content into soundscapes. Starting with images, the AI generates a textual description which is then used to create an evocative sonic environment, simulating a soundscape.
 	     
Sight has always played a key role in human survival. In modern times, it has taken a predominant place in the development of technologies, leading to the assumption that the other senses are less important.

Hearing is often overlooked, despite its important role today in the entertainment industry. 
This presence is, nonetheless, mostly associated with visual inputs, such as images, videos, or more general aesthetics that help make the sound recognizable to audiences.

The goal of this project is to investigate how deep learning models interpret images and how this interpretation manifests acoustically, offering insights into how visual stimuli can influence auditory imagination in AI systems.


Objectives

•	Investigate the relationship between visual and auditory perception.

•	Assess the semantic and sensory coherence between images and the corresponding soundscapes.

•	Creation of a user-friendly web interface

Tools and Technologies

•	[Google Colab](https://colab.research.google.com/drive/1_zZ2UCEBYPDsA7v3CW4ic-yPSMhY440r#scrollTo=01e--5cHKO_r): To manage and run the entire workflow. (Link is a draft of what the pipeline might look like, very rough draft)

•	Python + AI Libraries: transformers, diffusers, torch, torchaudio

•	[CLIP](https://huggingface.co/openai/clip-vit-large-patch14) or [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base): for image captioning

•	[MusicGen from Facebook](https://huggingface.co/spaces/facebook/MusicGen) (or another text-to-audio model, depending on results) 

•	[Gradio](https://www.gradio.app)/[Firebase studio](https://firebase.google.com): To build a web interface.

•	[Hugging Face](https://huggingface.co/models): For pre-trained models.


AI Techniques Employed

•	Vision-to-Text: Multimodal models (BLIP, CLIP) to generate a textual description from an image.

•	Text-to-Audio: Models like [MusicGen](https://huggingface.co/spaces/facebook/MusicGen) and trials with other different Hugging Face-based models for synthesizing audio from textual prompts.
Expected Outcomes

•	Soundscapes: Generated from images, providing a listening experience for each visual input.

•	[Gradio interface](https://www.gradio.app)/[Firebase studio](https://firebase.google.com): for interactively navigating the process of generating soundscapes from images (optional)

•	A final report with results analysis, critical reflections, and observations on how the AI models handle the transformation from image to sound.


![Alt text](https://github.com/napstablook911/From-Vision-to-Sound-GenAI-Explorations-Between-Image-and-Soundscape/blob/main/15AD22E0.png#:~:text=Files%C2%A0mainAdd%20filet-,15AD22E0.png)

![Alt text](https://github.com/napstablook911/From-Vision-to-Sound-GenAI-Explorations-Between-Image-and-Soundscape/blob/main/7D900561.png#:~:text=Files%C2%A0mainAdd%20filet15AD22E0.png-,7D900561.png)
 

